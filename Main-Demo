"""
Title: 但丁研究中的数字人文 Digital Humanities in Dante Studies
Date: 2019-11-17
Author：成 沫 Silvano Mo Cheng   同济大学人文学院、欧洲思想文化研究院
Contact: silvano.cheng@gmail.com


  _____  _       _ _        _          _    _                             _ _   _
 |  __ \(_)     (_) |      | |        | |  | |                           (_) | (_)
 | |  | |_  __ _ _| |_ __ _| |        | |__| |_   _ _ __ ___   __ _ _ __  _| |_ _  ___  ___
 | |  | | |/ _` | | __/ _` | |        |  __  | | | | '_ ` _ \ / _` | '_ \| | __| |/ _ \/ __|
 | |__| | | (_| | | || (_| | |        | |  | | |_| | | | | | | (_| | | | | | |_| |  __/\__ \
 |_____/|_|\__, |_|\__\__,_|_|        |_|  |_|\__,_|_| |_| |_|\__,_|_| |_|_|\__|_|\___||___/
  _         __/ |      _____              _                 _____ _             _ _
 (_)       |___/      |  __ \            | |               / ____| |           | (_)
  _ _ __              | |  | | __ _ _ __ | |_ ___         | (___ | |_ _   _  __| |_  ___  ___
 | | '_ \             | |  | |/ _` | '_ \| __/ _ \         \___ \| __| | | |/ _` | |/ _ \/ __|
 | | | | |            | |__| | (_| | | | | ||  __/         ____) | |_| |_| | (_| | |  __/\__ \
 |_|_| |_|            |_____/ \__,_|_| |_|\__\___|        |_____/ \__|\__,_|\__,_|_|\___||___/


#    _            ___ _                    __  __
#   | |__ _  _   / __| |_  ___ _ _  __ _  |  \/  |___
#   | '_ \ || | | (__| ' \/ -_) ' \/ _` | | |\/| / _ \
#   |_.__/\_, |  \___|_||_\___|_||_\__, | |_|  |_\___/
#         |__/                     |___/



,,,,,,,,,,,,,,,,,,,,,,,,,,,,**,,,,***#@(,,*#%@#**,(%&@@@@@@@&(,................
,,,,,,,,,,,,,*,,*,,,,,*/(#%%%%%%%%%&@@@&&@@@@@%#@@@@@@@@@@@@,,,................
,,,,,,,,,,,,,,*(%&&%%%%%%%%&&%&%&&@@@@@@@@@@@@@@@@@@@@@@@&*,,,,............. ..
,,,,,,,,,,(%&%%%&&&&&%%%%%&@&%&&&@@@@@@@@@@@@@@@@@@@@@@@/,,.,..................
,,,,,,.,#&%%%%&&%%%%%%%%%%@@@&&&&@@@@@@@@@@@@@@@@@@@@@@@@@/....................
,,,,,,&&&&&&&%%%%%%%%%%%%@@@@&&&&@@@@@@@@@@@@@&*,(&@@@@@@@*..................
,,.,(&&%%&&&%%%%%%%%%%%%@@@@@&&&&@@@@@@@@@@@&%(/*/.,,,*/((((,..................
,..&&&&%%&%%%%%%%%%%%%%&@@@@@@&&@@&&%%%%%%###(//**/,,,,,,,,,,,.................
,,%&&&%%%%%%%%%%&%%%%&%@@@@@@&@@&&%%%%%%###(((/****/,.,,.,,,.,.................
,/@&&&%%%%%%%%%%@@&%%&&@@@@@@@@@@&&@@@@@%##((((##%#/*,,,,,,,,,,,...............
*&&&&%%&&%%%%%%%@@@@&&&@@@@@@@@@@@@@@@####%%////%&&/,,,,,,,,.................
(&&&%%%&%%%%%&&&@@@@&&&@@@@@@@@@@@@@@######(//(#%%%/...,,,,,.................
&&&&&%&&&%%&&&&&@@@@&&@@@@@@@@@@@@@&@%#(####((#%(/(#(/,,,..,...................
&&&&&&&&&&@&&&&@@@@@@@@@@@@@@@@@&%&@&%####(((%%%#%(##(/*,......................
@&&&&&@&&&@@@&@@@@@@@@@@@@&&%%%%%&@&%#####(//###(#(((##/*,.....................
&&&&&&&&&@@@@&@@@@@@@@@&&&&%%%%#/@@&%##%###(/*///(//(##(/*.....................
&&&&&&&&@@@@@@@@@@@@@@@@@@@@@&(*.&&%#%%%%##((///((//((##(/,....................
&&&&&@&@@@@@@@@@@@@@@@@@@@@@@@@@//######%%###((((((/#(/(#(/,....,..............
&&&&&@&@@@@@@@&@@@@@@@@@@@@@&&@@@.#########%###((#(/#&%##(/*...................
&&&&&@&@@@@@@&@@@&&&@&&&&%/*,,,,(.,####(######((((/*/#%#%%(,............ . ....
&&&&&@&&@@@@@@@&&&&&&%/*,........,###(((((((((((*///(,,..,....  .....  . .. .
&&&&&@&&&@@@@@@@@@@@&%(/*,..........####(((((((((*(((%(,......... .  .     ...
&&&&&&&&&@@@@@@@@@@@@@@#**..  . ....,##((((((#((//%%###,..........  ..... ..
&&&&&&&&%@@@@@@@@@@@@@@@&&/........../##((((####(/(((#*,.......       .    .
@&&&&&&%%@@@@@@@@@@@@@&%%%%%%##(*,,,.*%###(((###(//////,......... .         ...
&&&&&&&&&&@@@@@@@&&%%%%#######%%####**%%#########((((///.........      ..  ....
%&&&&&&&&@@@&&&&&%%%###%###########%(,#%######%#####((#*.......  ..     .   ...
/&&&&&&&&@@%%%%%%%%%################(,#%%%/....,,****,.......     ..       ....
,(@&&&&&&@%%%%%%%%%#############((((*,((#*.............. ...  ... .   .     ...
,,%&&&&&&%%%%%%%%%%%%%%(((#######(((/,//(,.,............  .. .. ...        ....
,,,&&&@&%%%%%%%%%%%%%%%%%%%#(/(#((((*,//(/*,................       ...      .
,,,#@&&%%%%%%%%%%%%%%%%%%%%%%%##(////*(/(#/..................  ....
,,,,#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%#*%###/..........................      . .
,,(%%%%%%%%%%%%#%#%%%%%%%%%%%%%%%%%%#*(####(,.........................      .
#%%%%%%%%%%%%%#############%%%%%%%%%%(/######(.......................        ..
%%%%%%%%%%%%#%%#######################/(###(#((/, ...................   .
%%%%%%%%######%%###################((((*((#(#((((/.....................
%%%%%%%%#####(((##%%#################(((/#(((((((((*.................    .
%%%%%%%%%######(((((###################((/(((((((((((,............ ..
%%%%%%%%%#######(#((###%%###########((((((((((((##((((*.............     .
%%%%%%%%###########(((#%%%############((((((((((####((((,.......        ..  .
%%%%%%%%%#########(((((#%%#######(####(((((#(((((#(((((((*......    ...
%%%%%%%%########((((((((##%%############(((##(((((((((((((/,.....  ....
%%%%%%%%#########(((((((((%%%###########(((###(((((#((((((((,..... ..   .
%%%%%%%%###########(((((((##%###########((((((#((((((((((((((*....   .
%%%%%%%############((((((((#%%%%%#########(#((#(((((((#((((((((...   .    .
%%%%%%%%###########((((((((##%%%%#####%####((((((((((((#(((((((/...
%%%%%%%%%%##########((((((####%%%%#########(((((#(((((((#((((((((,....
%%%%%%%%%#############((((####%%%%%#########(((((#(((((((((((((((#,... ..
%%%%%%%%%######%######((((####%%%%%%#########(((((#(#((((((((((((((,.....
%%%%%%%%%#############(#(((####%%%%%#####%#####(((#####(((((#(((((((*... .
%%%%%%%%%%%##%#########((((####%%%%%#####%%#####(####(##(((((((((((#(....  .
&%%%%%%%%%%%%###############%%#%%%%%%%####%%#####(#######(((((#((((((/... .
&%%%%%%%%%%%%###%%%##########%##%%%%%%%####%#####((((##(##(((((##((#(#/.....

"""


import nltk, string, jieba,jieba.analyse, matplotlib.pyplot as plt
from nltk.tokenize import sent_tokenize, word_tokenize
from PIL import Image
from wordcloud import WordCloud


dante = "mat/commedia-ita.txt"
dante_false = "mat/commedia-ita-false.txt"
petrarca = "mat/petrarca.txt"
ariosto = "mat/Orlando_furioso.txt"
huangGB = "mat/HuangGB.txt"
huangWJ = "mat/HuangWJ.txt"
diary = "mat/zasi.txt"
life = "Vita Brevis, Ars Longa\n\nLife is Short, I use Python."

def open_image(image_name):
    image = Image.open(image_name)
    image.show()


def open_file_in_line(txt):
    """打开文件，以每行为格式，储存为list"""
    out_list = []
    with open(txt) as f:
        for line in f:
            out_list.append(line)
    return out_list


def word_cloud_eng(txt):
    # Read the whole text.
    text = open(txt).read()

    # Generate a word cloud image
    wordcloud = WordCloud().generate(text)

    # Display the generated image:
    # the matplotlib way:

    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")

    # lower max_font_size
    wordcloud = WordCloud(max_font_size=40).generate(text)
    plt.figure()
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

    """
    # The pil way (if you don't have matplotlib)
    image = wordcloud.to_image()
    image.show()
    """


def word_freq_cn(text,name,n):
    txt = open(text, 'r', encoding='utf-8').read()
    txt = jieba.lcut(txt)
    counts = {}  # count dictionary
    for word in txt:
        if len(word) == 1:   # 去掉单个词
            continue
        else:
            rword = word
        counts[rword] = counts.get(rword, 0) + 1
    li = list(counts.items())
    li.sort(key=lambda x: x[1], reverse=True)
    for i in range(1, n):
        key, value = li[i]
        print('{:<3}{:<6}{:>5}'.format(i, key, value))
    print('以上为', name, '  神曲前',n,'个最高频词')


def jieba_analyse(text):
    txt = open(text, 'r', encoding='utf-8').read()
    tags = jieba.analyse.extract_tags(txt, topK=30)
    print (u"前20:")
    print (" ".join(tags))


def get_rhyme_list(tlst):
    """获取（由行组成的list）的最后一个词（韵脚）"""
    rhyme_lst = []
    for i in tlst:
        words = word_tokenize(i)
        words = [word.lower() for word in words if word.isalpha()]
        if len(words)>=3 and len(words) <= 12:  # 如果不是空list，或canto I 之类的说明性文字
            rhyme_lst.append(words[-1])
    return rhyme_lst


def get_rhyme_group_list(rhyme_lst):
    rhyme_group_list = []
    for single_rhyme_pos in range(len(rhyme_lst)-2):
        word = rhyme_lst[single_rhyme_pos]
        rhyme_element = word[-3:]     # 0  尾韵最后三个字母相同，即通常情况下的rima perfetta
        #rhyme_element = word[-2:]     # 1: rima siciliana的可能性
        #rhyme_element = word[-5:]     # 2： 若改成word[-5:]则rima piu' rigida
        #rhyme_element = word          # 3： rima identica
        target_word = rhyme_lst[single_rhyme_pos+2]
        if rhyme_element == target_word[-3:]:      # 0
        #if rhyme_element == target_word[-2:]:     # 1: rima siciliana的可能性
        #if rhyme_element == target_word[-5:]:     # 2
        #if rhyme_element == target_word:          # 3： rima identica
            word1 = rhyme_lst[single_rhyme_pos]
            word2 = rhyme_lst[single_rhyme_pos+1]
            word3 = rhyme_lst[single_rhyme_pos+2]
            group_of_words = [word1, word2, word3]
            rhyme_group_list.append(group_of_words)
    print('rhyme_group_list length: ', len(rhyme_group_list))
    
    return rhyme_group_list

def get_rhyme_group_list1(rhyme_lst):
    """
    不用
    :param rhyme_lst:
    :return:
    """
    rhyme_group_list = []
    for single_rhyme_pos in range(len(rhyme_lst)-2):
        word = rhyme_lst[single_rhyme_pos]
        #rhyme_element = word[-3:]       # 尾韵最后三个字母相同，即通常情况下的rima perfetta
        rhyme_element = word[-2:]     # 1: rima siciliana的可能性
        #rhyme_element = word[-5:]     # 2： 若改成word[-5:]则rima piu' rigida
        #rhyme_element = word          # 3： rima identica
        target_word = rhyme_lst[single_rhyme_pos+2]
        #if rhyme_element == target_word[-3:]:
        if rhyme_element == target_word[-2:]:     # 1: rima siciliana的可能性
        #if rhyme_element == target_word[-5:]:     # 2 
        #if rhyme_element == target_word:          # 3： rima identica
            word1 = rhyme_lst[single_rhyme_pos]
            word2 = rhyme_lst[single_rhyme_pos+1]
            word3 = rhyme_lst[single_rhyme_pos+2]
            group_of_words = [word1, word2, word3]
            rhyme_group_list.append(group_of_words)
    print('rhyme_group_list length: ', len(rhyme_group_list))
    return rhyme_group_list


def get_rhyme_group_list3(rhyme_lst):
    """
    不用
    :param rhyme_lst:
    :return:
    """

    rhyme_group_list = []
    for single_rhyme_pos in range(len(rhyme_lst)-2):
        word = rhyme_lst[single_rhyme_pos]
        rhyme_element = word[-3:]       # 尾韵最后三个字母相同，即通常情况下的rima perfetta
        #rhyme_element = word[-2:]     # 1: rima siciliana的可能性
        #rhyme_element = word[-5:]     # 2： 若改成word[-5:]则rima piu' rigida
        rhyme_element = word          # 3： rima identica
        target_word = rhyme_lst[single_rhyme_pos+2]
        #if rhyme_element == target_word[-3:]:
        #if rhyme_element == target_word[-2:]:     # 1: rima siciliana的可能性
        #if rhyme_element == target_word[-5:]:     # 2
        if rhyme_element == target_word:          # 3： rima identica
            word1 = rhyme_lst[single_rhyme_pos]
            word2 = rhyme_lst[single_rhyme_pos+1]
            word3 = rhyme_lst[single_rhyme_pos+2]
            group_of_words = [word1, word2, word3]
            rhyme_group_list.append(group_of_words)
    print('rhyme_group_list length: ', len(rhyme_group_list))
    return rhyme_group_list


def process_text(original_text, n, quant):
    """
    输出所有的韵脚，并展示自第num个开始的quant个韵脚
    """
    # 所有诗行,
    lines_list = open_file_in_line(original_text)

    # 所有韵脚
    result_rhyme = get_rhyme_list(lines_list)

    # 测试部分：
    print('源文件中的行数（包括空行）：', len(lines_list))
    print('总共韵脚数:', len(result_rhyme))


    for i in range(quant):
        print(result_rhyme[n])
        n += 1

    return result_rhyme


def get_frequency(txt, n):
    """获得最常见的n个词的词频，以list形式输出，"""
    wdict = {}  # a dictionary
    for word in txt:
        if word not in wdict:
            wdict[word] = 0
        wdict[word] += 1
    li = list(wdict.items())
    li.sort(key=lambda x: x[1], reverse=True)
    for i in range(1, n):
        key, value = li[i]
        print('{:<3}{:<6}{:>5}'.format(i, key, value))


def rhyme_group_test(txt, n):
    lines = open_file_in_line(txt)
    rhyme_list = get_rhyme_list(lines)
    rhyme_group = get_rhyme_group_list(rhyme_list)

    for i in rhyme_group[:n]:
         print(i)
    new_list = []

    for i in rhyme_group:
        new_str = i[0]+ ' ' + i[2]   # new_list.append(''.join(i))  一模一样的三个韵，不存在。
        new_list.append(new_str)

    return new_list


def print_lines_from_rhyme(single_rhyme):
    """"
    反向根据韵脚，寻找相对应的行数。
    """
    lines = open_file_in_line(dante)
    clear_lines = []
    for line in lines:
        words = word_tokenize(line)
        words = [word.lower() for word in words if word.isalpha()]
        if len(words)>=3 and len(words) <= 12:  # 如果不是空list，或canto I 之类的说明性文字
            clear_lines.append(words)
    for line_num in range(len(clear_lines)-2):
        if single_rhyme == clear_lines[line_num][-1]:
            print('')
            print(' '.join(clear_lines[line_num-2]),'\n',' '.join(clear_lines[line_num-1]),\
                  '\n',' '.join(clear_lines[line_num]),'\n',' '.join(clear_lines[line_num+1]), \
                  ' \n', ' '.join(clear_lines[line_num+2]))

def compare_siciliano():
    """
    比较两种韵尾计算方式，筛选特殊韵尾。
    :return:
    """
    lines = open_file_in_line(dante)
    rhyme_list = get_rhyme_list(lines)
    result_list1 = get_rhyme_group_list(rhyme_list)
    sicilian_list = get_rhyme_group_list1(rhyme_list)
    siciliano = []
    for i in sicilian_list:
        if i not in result_list1 and len(i[0]) > 3:
            siciliano.append(i)
            print(i)
    #print(" \n".join(siciliano.items()))
    print ("")




"""
1. 花里胡哨的展示--用BEATRICE写的浙大logo
"""
#open_image("mat/logo_ZJU.png")


"""
2. 远距离的阅读：Moretti: Distant Reading
"""
#open_image("mat/distant_reading_pic.jpg")

"""
3. 简单粗暴的词频分析法：简单粗暴
    3.1 但丁与彼特拉克 词频
"""
#word_cloud_eng(dante)
#word_cloud_eng(petrarca)

"""
    3.2 中文译本
    黄国彬、黄文捷译本
"""
#word_freq_cn(huangGB,"黄国彬译本",20)
#print('')
#word_freq_cn(huangWJ,"黄文捷译本",20)


"""
4. 诗歌韵脚
    4.1 concordanza韵脚索引
        4.1.1 Dante
"""

#process_text(dante,0,25) # 从第0号开始，接下来的25个尾韵依次是
#get_frequency(process_text(dante_false,0,0),20)
#get_frequency(process_text(dante,0,0),20)

"""
        4.1.2 Petrarca、Ariosto 常用韵脚
"""
#get_frequency(process_text(petrarca,0,0),20)
#get_frequency(process_text(ariosto,0,0),20)


"""
5. Rimario Dantesco
提示：注意看rhyme_group_list函数,理解rima的分类
"""
#result_list = rhyme_group_test(ariosto,30)  # 通过修改参数实现rima分类
#print("")
#print_lines_from_rhyme("suso")  # 根据给定韵脚，给出对应诗行
#compare_siciliano() #探索不完美的韵脚
#print("")
#get_frequency(result_list,20)


"""

Hello World

"""

#print(life)
#open_image("mat/life.jpg")










